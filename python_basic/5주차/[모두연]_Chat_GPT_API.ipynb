{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4MN7dYuCvoF",
        "outputId": "9135d2b7-9632-43b0-b8d7-b52a604d3661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.9.0-py3-none-any.whl (223 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.4/223.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.14)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typing-extensions, h11, httpcore, httpx, openai\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 openai-1.9.0 typing-extensions-4.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI  # OpenAI 모듈 불러오기\n",
        "\n",
        "# OpenAI API 키 설정\n",
        "client = OpenAI(api_key='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')  # 사용자의 API 키로 대체해야 함\n",
        "\n",
        "# ChatGPT를 사용한 텍스트 생성 요청\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-3.5-turbo\",\n",
        "    messages = [{\"role\" : \"user\", \"content\" : \"Hello World!\"}]\n",
        ")\n",
        "# API 응답에서 마지막 메시지의 내용을 출력\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCgQ1yfyC2lQ",
        "outputId": "8877732f-d765-4cc5-8287-1bd616f83767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# ChatGPT를 사용한 텍스트 생성 요청\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-3.5-turbo\",\n",
        "    messages = [{\"role\" : \"user\", \"content\" : \"Translate the following English text to Korean: 'Hello, how are you?'\"}]\n",
        ")\n",
        "\n",
        "#응답 출력\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8zPF_R8EQwF",
        "outputId": "ed8d4f15-5326-4514-9a84-f5af482fb765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "안녕하세요, 어떻게 지내세요?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    response = client.chat.completions.create(\n",
        "        model = \"gpt-3.5-turbo\",\n",
        "        messages = [{\"role\" : \"user\", \"content\" : \"Translate the following korean text to English: 안녕하세요 오늘 날씨가 좋네요. 햇살이 맑아요\"}],\n",
        "        max_tokens=10,\n",
        "        temperature=0.7,\n",
        "        top_p=0.8,\n",
        "        frequency_penalty=0.2\n",
        "    )\n",
        "    print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K762cceDPtrc",
        "outputId": "9b4f6978-9b8e-4190-85a5-f251a150169c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, the weather is nice today. The sunlight\n",
            "Hello, the weather is nice today. The sunlight\n",
            "Hello, today the weather is good. The sunlight\n",
            "Hello, today's weather is nice. The sunlight\n",
            "Hello, today's weather is good. The sunlight\n",
            "Hello, today's weather is nice. The sunlight\n",
            "Hello, today's weather is nice. The sunlight\n",
            "Hello, the weather is nice today. The sunlight\n",
            "Hello, the weather is nice today. The sunlight\n",
            "Hello, today's weather is good. The sunlight\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_chatbot(messages):\n",
        "    response = client.chat.completions.create(\n",
        "        model = \"gpt-3.5-turbo\", messages = messages\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "prompt_role = \"너는 블로그 전문가, 파워블로그처럼 글을 써야해.\\\n",
        "                개발자의 직업관에 대한 글을 써야하고,\\\n",
        "                그리고 취업을 준비하는 20대 독자들에게 잘 보일수 있도록 글을 써야되\\\n",
        "                SEO최적화된 글을 써야되\""
      ],
      "metadata": {
        "id": "FSF0p8ymQQof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "def assist_blogger(\n",
        "    facts: List[str], tone: str, length_words: int, style: str\n",
        "):\n",
        "    facts = \", \".join(facts)\n",
        "    prompt_role = \"너는 블로그 전문가고, 파워블로그처럼 글을 써야해\"\n",
        "    prompt = f\"{prompt_role} \\\n",
        "            FACTS: {facts} \\\n",
        "            TONE: {tone} \\\n",
        "            LEGNTH: {length_words} words \\\n",
        "            STYLE: {style}\"\n",
        "    return ask_chatbot([{\"role\": \"user\", \"content\": prompt}])"
      ],
      "metadata": {
        "id": "WGXxuH_zQokH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    assist_blogger(\n",
        "        [\"대학 진학 이후의 개발자의 삶은?\"], \"informal\", 100, \"blogpost\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46Zrj3wIQz3E",
        "outputId": "9489bfea-c253-43ae-d16c-b4362e1d984b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어서와! 파워블로그처럼 글을 써야한다니? 그러면 지금부터 대학 진학 이후의 개발자의 삶에 대해 이야기해보자.\n",
            "\n",
            "대학을 졸업하고 개발자로 취업했다면, 어떤 일들이 기다리고 있을까? 사실 대학 시절과는 다른 모습을 보일지도 몰라. 물론 대학에서 배운 개념과 이론들은 막강한 기반이 될 것이다. 하지만 실무에서는 단순한 코드만 작성하는 것이 아니라 프로젝트의 전반을 이해하고 참여할 필요가 있다.\n",
            "\n",
            "때로는 긴장감과 압박감도 느낄 수 있는데, 이는 프로젝트에 대한 책임감 때문이다. 또한, 기술적인 도전도 많이 경험하게 될 것이다. 그러니까 계속해서 새로운 언어나 프레임워크를 익히고, 스킬을 향상시키는 자세를 갖도록 노력해야 한다는 것이 중요하다.\n",
            "\n",
            "또한, 팀원들과의 협업은 개발자로서 성장하는 데 핵심적인 역할을 한다. 다양한 의견을 수렴하고 이해하며 함께 문제를 해결하는 과정을 통해 자신을 발전시킬 수 있다.\n",
            "\n",
            "하지만 개발자의 삶은 언제나 로맨틱하지만도 않다. 무수히 많은 버그들과 에러들을 마주하게 될 것이다. 하지만 이러한 문제들은 능력을 키워나가는 데 필수적인 요소이기도 하다. 따라서, 문제 해결 능력을 키우는 것도 중요한 부분이다.\n",
            "\n",
            "마지막으로, 개발자의 삶은 한 번도 멈추지 않는 학습의 연속이라는 사실을 알아두자. 새로운 기술들이 끊임없이 등장하고 발전하기 때문에, 계속해서 배움의 자세를 유지하는 것이 필수적이다. 끊임없이 배움에 대한 열정을 가지고 새로운 도전에 맞서면 개발자로서의 성장은 언제나 가능하다는 것을 잊지마!\n",
            "\n",
            "대학 진학 이후의 개발자의 삶은 힘들기도 하지만, 보람도 가득하다. 실력을 키우기 위한 자기계발과 자신을 믿는 자세를 갖추는 것이 가장 중요하다. 기회와 도전은 계속 찾아오니, 우리 모두 계속해서 발전하도록 노력해보자!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    assist_blogger(\n",
        "        facts =[\"Chat GPT 등장이후의 취업 전략은?\"],\n",
        "        tone = \"정중하게\",\n",
        "        length_words = 200,\n",
        "        style= \"파워블로그 스럽게\"\n",
        "        )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-0Tml7IRFlR",
        "outputId": "b8400b11-bfb5-47eb-cdb3-7780fbb853c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "안녕하세요! 저는 블로그 전문가이며, 파워블로그처럼 글을 쓸 수 있습니다. 오늘은 Chat GPT 등장 이후의 취업 전략에 대해 알려드리도록 하겠습니다.\n",
            "\n",
            "Chat GPT는 최근 자연어처리 기술의 발전으로 인하여 많은 인기를 끌고 있습니다. 이러한 기술의 등장으로 인해 챗봇, 가상비서, 자동번역 등 다양한 분야에서 활용되고 있으며, 이에 따라 취업 전략도 변화하고 있습니다.\n",
            "\n",
            "첫 번째로, Chat GPT를 적극적으로 활용해야 합니다. 이는 채용 과정에서 인터뷰를 대신할 수 있을 뿐만 아니라, 인공지능 기술에 대한 이해도를 보여줄 수 있다는 점에서 큰 장점입니다. 이를 통해 경험과 역량을 강조하며, 기술적으로 유능함을 어필하는 것이 좋습니다.\n",
            "\n",
            "두 번째로, 개인 블로그를 운영하는 것도 좋은 전략입니다. Chat GPT 관련 포스트를 작성하면서 자신의 경험과 이해도를 공유하면 되는데, 이를 통해 꾸준한 업데이트와 공유를 통해 실력을 더욱 향상시킬 수 있습니다.\n",
            "\n",
            "세 번째로, 온라인 커뮤니티 활동에 적극적으로 참여하는 것이 중요합니다. Chat GPT와 관련된 온라인 커뮤니티에서 질문에 답변하거나 의견을 나누는 방식으로 인기를 얻을 수 있으며, 이를 통해 더 많은 사람들과 교류하고 네트워킹을 할 수 있습니다.\n",
            "\n",
            "Chat GPT 등장 이후의 취업 전략은 오늘 소개한 세 가지 요소를 중심으로 구성될 수 있습니다. 챗봇 기술은 더욱 발전할 것이므로, 관련 분야에서의 자신의 경쟁력을 높이기 위해 지속적인 노력과 발전이 필요합니다. 감사합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/[English] But what is a neural network_ _ Chapter 1, Deep learning [DownSub.com].txt\", \"r\") as f:\n",
        "    transcript = f.read()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-3.5-turbo\",\n",
        "    messages = [\n",
        "        {\"role\" : \"system\", \"content\" : \"너는 유튜브를 영어에서 한국어로 번역하는 번역가이자, 요약을 잘하는 역할을 할꺼야\"},\n",
        "        {\"role\" : \"user\", \"content\" : \"업로드한 파일을 한국어로 변역해줘\"},\n",
        "        {\"role\" : \"assistant\", \"content\" : \"Yes.\"},\n",
        "        {\"role\" : \"user\", \"content\" : \"한국어로 번역한 내용을 요약해\"},\n",
        "        {\"role\" : \"assistant\", \"content\" : \"Yes.\"},\n",
        "        {\"role\" : \"user\", \"content\" : transcript}\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "lx5qKvtVTDwT",
        "outputId": "8b5217eb-9a0a-4a41-9aa5-152da58eb9f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 4119 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-5bf192bcdb5d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtranscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     messages = [\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 648\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    649\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1177\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         )\n\u001b[0;32m-> 1179\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 868\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    869\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 4119 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 첨부된 파일을 읽고 작은 부분으로 나누기 위한 코드입니다.\n",
        "\n",
        "# 파일 경로 설정\n",
        "file_path = '/content/[English] But what is a neural network_ _ Chapter 1, Deep learning [DownSub.com].txt'\n",
        "\n",
        "# 파일을 읽어서 내용을 저장\n",
        "with open(file_path, 'r') as file:\n",
        "    transcript = file.read()\n",
        "\n",
        "# 텍스트를 나눌 최대 길이 설정 (토큰 수가 아닌 문자 수 기준)\n",
        "max_length = 5000  # 각 부분의 최대 길이 (문자 수)\n",
        "\n",
        "# 텍스트를 작은 부분으로 나누는 함수\n",
        "def split_into_parts(text, length):\n",
        "    return [text[i:i+length] for i in range(0, len(text), length)]\n",
        "\n",
        "# 텍스트를 여러 부분으로 나눔\n",
        "parts = split_into_parts(transcript, max_length)\n",
        "\n",
        "# 나누어진 부분들의 수와 첫 부분의 내용 일부를 출력\n",
        "num_parts = len(parts)\n",
        "first_part_preview = parts[0][:500]  # 첫 부분의 처음 500자\n",
        "\n",
        "num_parts, first_part_preview"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iweD1LezUDw8",
        "outputId": "89c26f94-5c28-4597-e8e6-4dd6325a11d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,\n",
              " \"This is a 3. It's sloppily written and rendered at an extremely low resolution of 28x28 pixels,\\n\\nbut your brain has no trouble recognizing it as a 3. And I want you to take a moment\\n\\nto appreciate how crazy it is that brains can do this so effortlessly. I mean, this,\\n\\nthis and this are also recognizable as 3s, even though the specific values of each pixel\\n\\nis very different from one image to the next. The particular light-sensitive cells in your\\n\\neye that are firing when you see this 3 are very \")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫 번째 부분만 사용\n",
        "first_part = parts[0]\n",
        "\n",
        "# 첫 번째 부분에 대한 번역 요청\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages = [\n",
        "        {\"role\" : \"system\", \"content\" : \"너는 유튜브를 영어에서 한국어로 번역하는 번역가이자, 요약을 잘하는 역할을 할꺼야\"},\n",
        "        {\"role\" : \"user\", \"content\" : \"업로드한 파일을 한국어로 변역해줘\"},\n",
        "        {\"role\" : \"assistant\", \"content\" : \"Yes.\"},\n",
        "        {\"role\" : \"user\", \"content\" : \"한국어로 번역한 내용을 요약해\"},\n",
        "        {\"role\" : \"assistant\", \"content\" : \"Yes.\"},\n",
        "        {\"role\" : \"user\", \"content\" : first_part}\n",
        "      ],\n",
        ")\n",
        "\n",
        "# 번역 결과 출력\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ct7gPEdUY36",
        "outputId": "92483dad-1daa-4f91-d2da-c9e246990cf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이것은 3입니다. 조잡하게 작성되었으며 28x28 픽셀의 매우 낮은 해상도로 렌더링되었습니다. 그러나 당신의 뇌는 이것을 3으로 인식하는 데 어려움이 없습니다. 그리고 이것, 이것, 이것도 3으로 인식됩니다. 각 이미지의 픽셀 값이 다르더라도 특정 빛에 민감한 눈의 세포는 이 3을 볼 때 활성화되는 세포와는 매우 다릅니다. 그러나 당신의 믿을 수 없을 만큼 똑똑한 시각 피질에서 어떤 것이 이를 동일한 개념으로 인식하고 동시에 다른 이미지를 고유한 개념으로 인식하는지를 해결합니다. 하지만 \"28x28의 그리드를 입력으로 받아 0과 10 사이의 단일 숫자를 출력해 입력된 숫자를 판단하는 프로그램을 작성해봐\"라고 말하면, 이 작업은 어렵기로 요리 to는 작고 어렵게 됩니다. 돌아돌아보지 않았다면, 기계학습과 신경망의 현대와 미래의 중요성과 중요성을 설득할 필요도 없을 것 같습니다. 하지만 여기서 내가 하고 싶은 것은 단순히 배경없이 신경망이 실제적으로 무엇인지를 보여주고, 그것이 수학의 한 부분으로서 어떤 것을 하는지를 시각적으로 도와드리는 것입니다. 내 희망은 단지 당신이 구조 자체가 타당하다고 생각하고 신경망이 \"학습\"의 용어로 읽거나 들을 때 그것이 무슨 의미인지 알게 되는 것입니다. 본 영상은 그 구조 요소에 전념 할 것이며, 다음 내용은 학습을 다룹니다.\n",
            "\n",
            "우리가 할 일은 손으로 쓴 숫자를 인식하는 데 학습할 수있는 신경망을 만드는 것입니다. 이 주제를 소개하는 데 약간 고전적인 예입니다. 이런 상태로 유지하는 것을 선택합니다. 두 비디오의 끝에서는 더 많이 배울 수 있는 몇 가지 좋은 자료와 이 작업을 수행하는 코드를 다운로드하여 고유한 컴퓨터에서 사용할 수있는 곳을 알려 드릴 예정입니다. 다양한 신경망이 있습니다. 최근 몇 년 동안이러한 변형에 대한 연구가 유난히 활발하게 이루어지고 있지만, 이 두 가지 입문 비디오에서는 당신과 나는 막대한 의문으로 숫자를 인식하는 이과정이 어떻게 처리될 것인지에 대해보다 잘 이해하기위해 가장 기본적인 평범한 형태에 대해서만 살펴볼 것입니다. 실제로는 이 특정 구조로 실험할 여지가 많이 있습니다. 네트워크의 작동 방식은 하나의 계층에서의 동작이 다음 계층의 동작을 결정합니다. 물론 정보 처리 메커니즘으로서의 네트워크의 핵심은 한 계층에서 다음 계층의 동작을 어떻게 가져오는지에 정확히 달려 있습니다. 이는 생물학적 뉴런 네트워크에서 일부 뇌의 그룹 격동이 특정 다른 그룹의 격동을 일으키는 것과 관련이 있다는 점에 대한 대략적인 유추를 의미합니다. 지금 여기에 표시된 네트워크는 이미 숫자를 인식하기 위해 훈련되었습니다. 저는 무엇을할 수 있는지 보여 드리겠습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OyajWDVaUoXF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}